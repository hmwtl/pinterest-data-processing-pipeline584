{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequeste including maven dependencies preinstalled by cloudformation.\n",
    "# start pyspark: spark/bin/pyspark \n",
    "\n",
    "import multiprocessing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from os.path import expanduser\n",
    "import os\n",
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pyspark.sql.functions as F\n",
    "from airflow.operators.bash import BashOperator\n",
    "from pathlib import Path\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark.sql.functions import when, regexp_replace, regexp_extract, col\n",
    "import requests\n",
    "from time import sleep\n",
    "import random\n",
    "from multiprocessing import Process\n",
    "import boto3\n",
    "import json\n",
    "import sqlalchemy\n",
    "import uuid\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "home = expanduser(\"~\")\n",
    "airflow_dir = os.path.join(home, 'airflow')\n",
    "assert os.path.isdir(airflow_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "home = expanduser(\"~\")\n",
    "airflow_dir = os.path.join(home, 'airflow')\n",
    "Path(f\"{airflow_dir}/dags\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ETL():\n",
    "\n",
    "    '''\n",
    "    Initialise and configure spark setting.\n",
    "    '''\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "            .setMaster(f\"local[{multiprocessing.cpu_count()}]\") \\\n",
    "            .appName(\"s3tospark\") \\\n",
    "            .getOrCreate()\n",
    "    # hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "    # hadoopConf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    # hadoopConf.set('fs.s3a.access.key',<your access key>)\n",
    "    # hadoopConf.set('fs.s3a.secret.key', <your secret key>)\n",
    "    # hadoopConf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loading data from aws s3 bucket\n",
    "    \"\"\"\n",
    "    df = spark.read.json(\"/home/kafka/Documents/pinterest_project/data/*.json\")\n",
    "\n",
    "    \"\"\"\n",
    "    Removing duplicates and replace error value with None value\n",
    "    \"\"\"   \n",
    "    \n",
    "    # remove duplicates\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # replace error cells with Nones        \n",
    "    df = df.replace({'No description available Story format': None}, subset = ['description'])\\\n",
    "                    .replace({'No description available': None}, subset = ['description'])\\\n",
    "                    .replace({'Image src error.': None}, subset = ['image_src'])\\\n",
    "                    .replace({'User Info Error': None}, subset = ['poster_name'])\\\n",
    "                    .replace({'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e': None}, subset=['tag_list'])\\ \n",
    "                    .replace({'No Title Data Available': None}, subset = ['title'])\\\n",
    "                    .replace({'User Info Error': \"0\"}, subset = ['follower_count'])# replace error values in follower_count with 0   \n",
    "\n",
    "    # replace error values with null\n",
    "    df = df.withColumn('save_location', regexp_replace('save_location', 'Local save in ', '')) \n",
    "    # drop the rows with null values in 'image_src' column, as we don't want a row without a pin\n",
    "    df = df.na.drop(subset=[\"image_src\"])\n",
    "\n",
    "    df = df.withColumn('follower_count', \n",
    "            when(df.follower_count.endswith('M'),regexp_replace(df.follower_count,'M','000000')) \\\n",
    "            .when(df.follower_count.endswith('k'),regexp_replace(df.follower_count,'k','000')) \\\n",
    "            .otherwise(df.follower_count)) \n",
    "\n",
    "    # cast follower_count column type as int\n",
    "    df = df.withColumn(\"follower_count\", F.col(\"follower_count\").cast(\"int\"))\n",
    "    # cassandra not allow use index as a column name, change \"index\" to \"idx\"\n",
    "    df = df.withColumnRenamed('index', 'idx')\n",
    "    # reorder selected columns\n",
    "    df = df.select('idx', 'title', 'poster_name', 'category', 'follower_count', 'description', 'image_src', 'is_image_or_video', 'tag_list', 'unique_id')\n",
    "\n",
    "\n",
    "\n",
    "def instantiate_cassandra():\n",
    "    # initial cassandra\n",
    "    # initialise cassandra driver\n",
    "    cluster = Cluster()\n",
    "    session = cluster.connect()\n",
    "    # create a cassandra keyspace\n",
    "    session.execute(\"CREATE KEYSPACE pinterest_project WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 3};\")\n",
    "\n",
    "    # create table\n",
    "    session.execute(\"CREATE TABLE pinterest(idx int PRIMARY KEY, title text, poster_name text, category text, follower_count int, description text, image_src text, is_image_or_video text, tag_list text, unique_id text);\")\n",
    "    # make preparedUpdate statements\n",
    "\n",
    "def sending_data_to_cassandra():\n",
    "    cluster = Cluster()\n",
    "    session = cluster.connect()\n",
    "    session.execute(\"USE pinterest_project;\")\n",
    "    preparedUpdate = session.prepare(\n",
    "        \"\"\" \n",
    "        INSERT INTO pinterest (idx, title, poster_name, category, follower_count, description, image_src, is_image_or_video, tag_list, unique_id) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    # write df to cassandra\n",
    "    for item in df.collect():\n",
    "        session.execute(preparedUpdate, [item[0], item[1], item[2], item[3], item[4], item[5], item[6], item[7], item[8], item[9]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ETL:\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialise and configure spark setting.\n",
    "        '''\n",
    "\n",
    "        findspark.init()\n",
    "        self.spark = SparkSession.builder \\\n",
    "                .setMaster(f\"local[{multiprocessing.cpu_count()}]\") \\\n",
    "                .appName(\"s3tospark\") \\\n",
    "                .getOrCreate()\n",
    "        hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "        hadoopConf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        hadoopConf.set('fs.s3a.access.key',<your access key>)\n",
    "        hadoopConf.set('fs.s3a.secret.key', <your secret key>)\n",
    "        hadoopConf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "\n",
    "\n",
    "    def load_data_from_aws_s3(self):\n",
    "        \"\"\"\n",
    "        loading data from aws s3 bucket\n",
    "        \"\"\"\n",
    "        self.df = self.spark.read.json(\"s3a://basicaccountstack-pinterestdataeng-proje-datalake-tcvpj2nf0cpq/pins/*.json\")\n",
    "        \n",
    "\n",
    "    def data_cleaning(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Removing duplicates and replace error value with None value\n",
    "        \"\"\"   \n",
    "        \n",
    "        # remove duplicates\n",
    "        self.df = self.df.dropDuplicates()\n",
    "\n",
    "        # replace error cells with Nones        \n",
    "        self.df = self.df.replace({'No description available Story format': None}, subset = ['description'])\\\n",
    "                        .replace({'No description available': None}, subset = ['description'])\\\n",
    "                        .replace({'Image src error.': None}, subset = ['image_src'])\\\n",
    "                        .replace({'User Info Error': None}, subset = ['poster_name'])\\\n",
    "                        .replace({'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e': None}, subset = ['tag_list']) \\ \n",
    "                        .replace({'No Title Data Available': None}, subset = ['title']) \\\n",
    "                        .replace({'User Info Error': \"0\"}, subset = ['follower_count'])\n",
    "        # replace error values with null\n",
    "        self.df = self.df.withColumn('save_location', regexp_replace('save_location', 'Local save in ', '')) \n",
    "        # drop the rows with null values in 'image_src' column, as we don't want a row without a pin\n",
    "        self.df = self.df.na.drop(subset=[\"image_src\"])\n",
    "    \n",
    "    def data_transformation(self):\n",
    "        self.df = self.df.withColumn('follower_count', \n",
    "                when(self.df.follower_count.endswith('M'),regexp_replace(self.df.follower_count,'M','000000')) \\\n",
    "                .when(self.df.follower_count.endswith('k'),regexp_replace(self.df.follower_count,'k','000')) \\\n",
    "                .otherwise(self.df.follower_count)) \n",
    "\n",
    "        # cast follower_count column type as int\n",
    "        self.df = self.df.withColumn(\"follower_count\", F.col(\"follower_count\").cast(\"int\"))\n",
    "        # cassandra not allow use index as a column name, change \"index\" to \"idx\"\n",
    "        self.df = self.df.withColumnRenamed('index', 'idx')\n",
    "        # reorder selected columns\n",
    "        self.df = self.df.select('idx', 'title', 'poster_name', 'category', 'follower_count', 'description', 'image_src', 'is_image_or_video', 'tag_list', 'unique_id')\n",
    "\n",
    "    def sending_data_to_cassandra(self):\n",
    "        # initial cassandra\n",
    "        # initialise cassandra driver\n",
    "        cluster = Cluster()\n",
    "        session = cluster.connect()\n",
    "        # create a cassandra keyspace\n",
    "        session.execute(\"CREATE KEYSPACE pinterest_project WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 3};\")\n",
    "        # initialise keyspace\n",
    "        session.execute(\"USE pinterest_project;\")\n",
    "        # create table\n",
    "        session.execute(\"CREATE TABLE pinterest(idx int PRIMARY KEY, title text, poster_name text, category text, follower_count int, description text, image_src text, is_image_or_video text, tag_list text, unique_id text);\")\n",
    "        # make preparedUpdate statements\n",
    "        preparedUpdate = session.prepare(\n",
    "            \"\"\" \n",
    "            INSERT INTO pinterest (idx, title, poster_name, category, follower_count, description, image_src, is_image_or_video, tag_list, unique_id) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # write df to cassandra\n",
    "        for item in self.df.collect():\n",
    "            session.execute(preparedUpdate, [item[0], item[1], item[2], item[3], item[4], item[5], item[6], item[7], item[8], item[9]])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=PythonOperator(\n",
    "    task_id='python_script',\n",
    "    python_callable=script.main,\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_from_s3 = [\n",
    "                PythonOperator(\n",
    "                task_id=\"read_data_from_s3\",\n",
    "                python_callable=_ETL.,\n",
    "                op_kwargs={\n",
    "                \"model\": model_id\n",
    "                }\n",
    "                ) for model_id in ['A', 'B', 'C']\n",
    "                ]\n",
    "choosing_best_model = BranchPythonOperator(\n",
    "                    task_id=\"choosing_best_model\",\n",
    "                    python_callable=_choosing_best_model\n",
    "                    )\n",
    "accurate = BashOperator(\n",
    "            task_id=\"accurate\",\n",
    "            bash_command=\"echo 'accurate'\"\n",
    "            )\n",
    "inaccurate = BashOperator(\n",
    "                task_id=\"inaccurate\",\n",
    "                bash_command=\" echo 'inaccurate'\"\n",
    "                )\n",
    "training_model_tasks >> choosing_best_model >> [accurate, inaccurate]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'Michael',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['h1m1w1@googlemail.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'start_date': datetime(2023, 1, 10),\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'end_date': datetime(2023, 1, 20),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'wait_for_downstream': False,\n",
    "    # 'dag': dag,\n",
    "    # 'trigger_rule': 'all_success'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set arguements\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2018, 9, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'schedule_interval': '@daily',\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(seconds=5),\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "  dag_id='my_dag', \n",
    "  description='Simple tutorial DAG',\n",
    "  default_args=default_args)\n",
    "\n",
    "src1_s3 = PythonOperator(\n",
    "  task_id='source1_to_s3', \n",
    "  python_callable=source1_to_s3, \n",
    "  dag=dag)\n",
    "\n",
    "src2_hdfs = PythonOperator(\n",
    "  task_id='source2_to_hdfs', \n",
    "  python_callable=source2_to_hdfs, \n",
    "  op_kwargs = {'config' : config},\n",
    "  provide_context=True,\n",
    "  dag=dag\n",
    ")\n",
    "\n",
    "src3_s3 = PythonOperator(\n",
    "  task_id='source3_to_s3', \n",
    "  python_callable=source3_to_s3, \n",
    "  dag=dag)\n",
    "\n",
    "spark_job = BashOperator(\n",
    "  task_id='spark_task_etl',\n",
    "  bash_command='spark-submit --master spark://localhost:7077 spark_job.py',\n",
    "  dag = dag)\n",
    "\n",
    "# setting dependencies\n",
    "src1_s3 >> spark_job\n",
    "src2_hdfs >> spark_job\n",
    "src3_s3 >> spark_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG(dag_id='pin_dag',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='*/1 * * * *',\n",
    "         catchup=False,\n",
    "         tags=['test']\n",
    "         ) as dag:\n",
    "    # Define the tasks. Here we are going to define only one bash operator\n",
    "    ETL = ETL()\n",
    "    \n",
    "    test_task = PythonOperator(\n",
    "        task_id='write_date_file',\n",
    "        bash_command='cd ~/Desktop && date >> ai_core.txt',\n",
    "        dag=dag)\n",
    "    \n",
    "    read_data_from_s3 = [\n",
    "                PythonOperator(\n",
    "                task_id=\"read_data_from_s3\",\n",
    "                python_callable=_ETL.load_data_from_aws_s3,\n",
    "\n",
    "                ]\n",
    "choosing_best_model = BranchPythonOperator(\n",
    "                    task_id=\"choosing_best_model\",\n",
    "                    python_callable=_choosing_best_model\n",
    "                    )\n",
    "accurate = BashOperator(\n",
    "            task_id=\"accurate\",\n",
    "            bash_command=\"echo 'accurate'\"\n",
    "            )\n",
    "inaccurate = BashOperator(\n",
    "                task_id=\"inaccurate\",\n",
    "                bash_command=\" echo 'inaccurate'\"\n",
    "                )\n",
    "training_model_tasks >> choosing_best_model >> [accurate, inaccurate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "def _choosing_best_model(ti):\n",
    "    accuracies = ti.xcom_pull(task_ids=[\n",
    "    'training_model_A',\n",
    "    'training_model_B',\n",
    "    'training_model_C'\n",
    "    ])\n",
    "    if max(accuracies) > 8:\n",
    "        return 'accurate'\n",
    "    return 'inaccurate'\n",
    "\n",
    "def _training_model(model):\n",
    "    return randint(1, 10)\n",
    "\n",
    "with DAG(\"my_dag\",\n",
    "    start_date=datetime(2021, 1 ,1), \n",
    "    schedule_interval='@daily', \n",
    "    catchup=False) as dag:\n",
    "\n",
    "    training_model_tasks = [\n",
    "        PythonOperator(\n",
    "            task_id=f\"training_model_{model_id}\",\n",
    "            python_callable=_training_model,\n",
    "            op_kwargs={\n",
    "            \"model\": model_id\n",
    "            }\n",
    "        ) for model_id in ['A', 'B', 'C']\n",
    "    ]\n",
    "\n",
    "    choosing_best_model = BranchPythonOperator(\n",
    "    task_id=\"choosing_best_model\",\n",
    "    python_callable=_choosing_best_model\n",
    "    )\n",
    "\n",
    "    accurate = BashOperator(\n",
    "    task_id=\"accurate\",\n",
    "    bash_command=\"echo 'accurate'\"\n",
    "    ) \n",
    "\n",
    "    inaccurate = BashOperator(\n",
    "    task_id=\"inaccurate\",\n",
    "    bash_command=\" echo 'inaccurate'\"\n",
    "    )\n",
    "training_model_tasks >> choosing_best_model >> [accurate, inaccurate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airflowRedditPysparkDag.py\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "'''\n",
    "input arguments for downloading S3 data \n",
    "and Spark jobs\n",
    "REMARK: \n",
    "Replace `srcDir` and `redditFile` as the full paths containing your PySpark scripts\n",
    "and location of the Reddit file will be stored respectively \n",
    "'''\n",
    "s3Bucket = '<YOUR_S3_BUCKET>'\n",
    "s3Key = '<YOUR_S3_KEY>'\n",
    "redditFile = os.getcwd() + '/data/RC-s3-2007-10'\n",
    "srcDir = os.getcwd() + '/src/'\n",
    "sparkSubmit = '/usr/local/spark/bin/spark-submit'\n",
    "\n",
    "## Define the DAG object\n",
    "default_args = {\n",
    "    'owner': 'insight-dan',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2016, 10, 15),\n",
    "    'retries': 5,\n",
    "    'retry_delay': timedelta(minutes=1),\n",
    "}\n",
    "dag = DAG('s3RedditPyspark', default_args=default_args, schedule_interval=timedelta(1))\n",
    "\n",
    "'''\n",
    "Defining three tasks: one task to download S3 data\n",
    "and two Spark jobs that depend on the data to be \n",
    "successfully downloaded\n",
    "task to download data\n",
    "'''\n",
    "downloadData= BashOperator(\n",
    "    task_id='download-data',\n",
    "    bash_command='python ' + srcDir + 'python/s3-reddit.py ' + s3Bucket + ' ' + s3Key + ' ' + redditFile,\n",
    "    dag=dag)\n",
    "\n",
    "#task to compute number of unique authors\n",
    "numUniqueAuthors = BashOperator(\n",
    "    task_id='unique-authors',\n",
    "    bash_command=sparkSubmit + ' ' + srcDir + 'pyspark/numUniqueAuthors.py ' + redditFile,\n",
    "    dag=dag)\n",
    "#Specify that this task depends on the downloadData task\n",
    "numUniqueAuthors.set_upstream(downloadData)\n",
    "\n",
    "#task to compute average upvotes\n",
    "averageUpvotes = BashOperator(\n",
    "    task_id='average-upvotes',\n",
    "    bash_command=sparkSubmit + ' ' + srcDir + 'pyspark/averageUpvote.py ' + redditFile,\n",
    "    dag=dag)\n",
    "averageUpvotes.set_upstream(downloadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pin_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa3c2183eefee0441b9afc282fff2c45f91aae1bdb1609dae94dd7f981e781ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
